{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lakehouse - incremental data load POC using Spark streaming\n",
        "#### We will utilise Spark structured streaming to implement our notebook for Dynamics customers data. Spark streaming has some powerful capabilities that handle incremental data with minimal setups. All new changes are read incrementally and merged into a target table"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from datetime import datetime\n",
        "from dateutil import parser,relativedelta\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.functions import year, month, dayofmonth, dayofweek, hour, to_date, col, quarter, explode, sequence, expr,current_timestamp,lit\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType, DoubleType, StringType, FloatType, ArrayType, LongType\n",
        "from delta.tables import DeltaTable\n",
        "from notebookutils import mssparkutils\n",
        "spark.conf.set(\"spark.sql.legacy.parquet.int96RebaseModeInWrite\",\"CORRECTED\")\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkNB",
              "session_id": "193",
              "statement_id": 48,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-01-02T04:02:30.2976196Z",
              "session_start_time": null,
              "execution_start_time": "2024-01-02T04:02:30.4249415Z",
              "execution_finish_time": "2024-01-02T04:02:30.5673897Z",
              "spark_jobs": null,
              "parent_msg_id": "04584a15-80ee-48d7-bba3-a8eb24824241"
            },
            "text/plain": "StatementMeta(SparkNB, 193, 48, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 47,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define variables\r\n",
        "##### `delta_load` flag defines whether its full load or incremental load from source.\r\n",
        "##### To run an initial full load or truncate/reload for a table, set `delta_load` flag as 0 and set the variable with name of the target table (for testing, define new target table). When `delta_load` is 0, it checks and removes the checkpoint directory if exists, before loading the data.\r\n",
        "##### To run an incremental load, set `delta_load` flag as 1.\r\n",
        "##### Define datalake, lakehouse, container and target table. \r\n",
        "##### <u>Make sure to create the lakehouse before this notebook is run.</u>"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0 for full load and 1 for incremental load\r\n",
        "delta_load=1 \r\n",
        "#delta_load=0\r\n",
        "\r\n",
        "# set path to source table - in our case storage account linked with synapse link\r\n",
        "path_to_source_table = \"abfss://yourcontainer@yourdatalake.dfs.core.windows.net/deltalake/vendtable_partitioned/\"\r\n",
        "\r\n",
        "#define lakehouse variables\r\n",
        "datalakename = \"yourdatalake\"\r\n",
        "container = \"yourcontainer\"\r\n",
        "lakehouse = \"lakehouse_1\" # target lakehouse - make sure you manually create it first\r\n",
        "target_table = \"vendtable_silver\" # target table\r\n",
        "\r\n",
        "# set path to target table\r\n",
        "path_to_target_table = f'abfss://{container}@{datalakename}.dfs.core.windows.net/{lakehouse}/{target_table}/'\r\n",
        "\r\n",
        "# set temp table and its path. This table holds incremental data before merging into target table\r\n",
        "temp_table = \"vendtable_temp\"\r\n",
        "lakehouse_temptable = lakehouse + \".\" + temp_table\r\n",
        "path_to_temp_table = f'abfss://{container}@{datalakename}.dfs.core.windows.net/{lakehouse}/{temp_table}/'\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkNB",
              "session_id": "193",
              "statement_id": 49,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-01-02T04:02:30.4166411Z",
              "session_start_time": null,
              "execution_start_time": "2024-01-02T04:02:30.6795875Z",
              "execution_finish_time": "2024-01-02T04:02:30.8197084Z",
              "spark_jobs": null,
              "parent_msg_id": "c0830eb7-a8cf-4cad-8395-a162ab64bbf7"
            },
            "text/plain": "StatementMeta(SparkNB, 193, 49, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 48,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpoints\r\n",
        "##### The core principle this pipeline uses to achieve incremental data captures is Spark streaming's checkpoints feature. We define a checkpoint folder location and with each run we commit a checkpoint, so the next run reads the checkpoint, like a watermark and picks up the newer delta files for load. The original intent behind checkpoints is to recover in case of a failure or intentional shutdown. This is to support structured streaming that runs 24/7. It allows the pipeline to recover the previous progress and state of a previous query, and continue where it left off. We configure a query with a checkpoint location, and the query will save all the progress information (i.e. range of offsets processed in each trigger) and the running aggregates (e.g. word counts in the quick example) to the checkpoint location. This checkpoint location has to be a path in an HDFS compatible file system (or Azure data lake, Amazon S3 etc) and can be set as an option in the DataStreamWriter when starting a query. More info: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#recovering-from-failures-with-checkpointing \r\n",
        "##### It is an easy, automated way of watermarking, which is a delta loading solution that loads the changed data between an old watermark and a new watermark. With checkpoints, we donâ€™t have to manage any control tables, version numbers or timestamps. Spark's checkpoint folders do that for us. More on watermarks: https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-overview#delta-data-loading-from-database-by-using-a-watermark"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define checkpoint location inside temp table folder\r\n",
        "checkpoint_dir = f\"{path_to_temp_table}/_checkpoint\""
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkNB",
              "session_id": "193",
              "statement_id": 50,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-01-02T04:02:30.5311618Z",
              "session_start_time": null,
              "execution_start_time": "2024-01-02T04:02:30.9613585Z",
              "execution_finish_time": "2024-01-02T04:02:31.099941Z",
              "spark_jobs": null,
              "parent_msg_id": "00d8621a-d0ae-457b-b94b-3761ecc05594"
            },
            "text/plain": "StatementMeta(SparkNB, 193, 50, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 49,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Data is loaded from the storage account associated with your Synapse link service. This is where your Dynamics data is exported. We load the data in dataframe `table_df` as a stream.\n",
        "##### Structured Streaming does not handle input that is not an append and throws an exception if any modifications occur on the table being used as a source. The option `ignoreChanges` brings in updates and deletes on top of inserts. Though in case of Dataverse, we get deleted records as new records with same Id and IsDelete=true. We handle duplicates before calling the SQL merge. For more: https://docs.delta.io/0.4.0/delta-streaming.html#ignoring-updates-and-deletes"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for efficiency, specify partitionId to read only small set of data\n",
        "# for eg, table_df = spark.readStream.format(\"delta\").option(\"ignoreChanges\",True).load(path_to_source_table).where(\"PartitionId=2023\")\n",
        "\n",
        "if delta_load == 0:\n",
        "    # Check if the checkpoint directory exists before removing it\n",
        "    if mssparkutils.fs.exists(checkpoint_dir):\n",
        "        mssparkutils.fs.rm(checkpoint_dir, True)\n",
        "    table_df = spark.readStream.format(\"delta\").option(\"ignoreChanges\",True).load(path_to_source_table)\n",
        "else:\n",
        "    table_df = spark.readStream.format(\"delta\").option(\"ignoreChanges\",True).load(path_to_source_table)\n",
        " "
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkNB",
              "session_id": "193",
              "statement_id": 51,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-01-02T04:02:30.6468359Z",
              "session_start_time": null,
              "execution_start_time": "2024-01-02T04:02:31.2600319Z",
              "execution_finish_time": "2024-01-02T04:02:31.397282Z",
              "spark_jobs": null,
              "parent_msg_id": "d3db0622-4175-4791-b92a-da08b95eecd9"
            },
            "text/plain": "StatementMeta(SparkNB, 193, 51, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 50,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " #####  We insert current timestamp in a column `lake_loadedtimestamp` to help note the timing"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "table_df = table_df.withColumn(\"lake_loadedtimestamp\", current_timestamp())"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkNB",
              "session_id": "193",
              "statement_id": 52,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-01-02T04:02:30.8990411Z",
              "session_start_time": null,
              "execution_start_time": "2024-01-02T04:02:31.5228696Z",
              "execution_finish_time": "2024-01-02T04:02:31.66387Z",
              "spark_jobs": null,
              "parent_msg_id": "8c3a62a1-2998-4e59-acff-c5d498512cc3"
            },
            "text/plain": "StatementMeta(SparkNB, 193, 52, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 51,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Define the function `writeToTempTable_inc` which writes current batch of data, from its dataframe into the temp table in delta format.\r\n",
        "##### Note we use Overwrite mode so previous changes are discarded and we only retain new batch of incremental data."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def writeToTempTable_inc (microbatchdf, batchid):\n",
        "    microbatchdf.write.format(\"delta\").option(\"overwriteSchema\", \"true\").saveAsTable(lakehouse_temptable, mode=\"overwrite\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkNB",
              "session_id": "193",
              "statement_id": 53,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-01-02T04:02:31.2791718Z",
              "session_start_time": null,
              "execution_start_time": "2024-01-02T04:02:31.7768362Z",
              "execution_finish_time": "2024-01-02T04:02:31.9128733Z",
              "spark_jobs": null,
              "parent_msg_id": "52da9528-2260-48e7-bb51-324594ea9eae"
            },
            "text/plain": "StatementMeta(SparkNB, 193, 53, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 52,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Writing incremental data in a temp table \n",
        "##### We use Spark's foreachbatch function to store incremental data captured in current run of the script to a temp table. Spark's foreachBatch(...) allows you to specify a function that is executed on the output data of every micro-batch of a streaming query. It takes two parameters: a dataFrame that has the output data of a micro-batch and the unique ID of the micro-batch. Note that calling the function takes no parameter but still it has two parameters specified in function `writeToTempTable_inc`. More info: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#foreachbatch\n",
        "\n",
        "##### Next we define, the trigger settings of our streaming query that defines the timing of streaming data processing, whether the query is going to be executed as micro-batch query with a fixed batch interval or as a continuous processing query. For our usecase, we use one time micro batch, that will process all the available data and stop on its own. More info: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#triggers\n",
        "\n",
        "##### CheckpointLocation is set. We start the query and await termination of streaming before proceeding to next step."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the streaming write operation with foreachBatch, checkpoint location and trigger once\n",
        "inc_Query = table_df.writeStream.format(\"delta\")\\\n",
        "    .foreachBatch(writeToTempTable_inc)\\\n",
        "    .trigger(once=True)\\\n",
        "    .option(\"checkpointLocation\", checkpoint_dir)\\\n",
        "    .start()\n",
        "\n",
        "# Await for the termination of the stream\n",
        "inc_Query.awaitTermination()\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkNB",
              "session_id": "193",
              "statement_id": 54,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-01-02T04:02:31.6944863Z",
              "session_start_time": null,
              "execution_start_time": "2024-01-02T04:02:32.0511086Z",
              "execution_finish_time": "2024-01-02T04:02:42.4968457Z",
              "spark_jobs": null,
              "parent_msg_id": "c4049325-5de4-4532-9a9f-0c3dcf9e4d4b"
            },
            "text/plain": "StatementMeta(SparkNB, 193, 54, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 53,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### At this point, temp table has the incremental data saved. Lets analyse."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.sql(f\"SELECT max(SinkModifiedOn), min(SinkModifiedOn) FROM {lakehouse_temptable}\")\n",
        "display(df)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkNB",
              "session_id": "193",
              "statement_id": 55,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-01-02T04:02:32.2198906Z",
              "session_start_time": null,
              "execution_start_time": "2024-01-02T04:02:42.6255235Z",
              "execution_finish_time": "2024-01-02T04:02:44.4341266Z",
              "spark_jobs": null,
              "parent_msg_id": "becda3fb-8620-4863-af30-e8acf3020e11"
            },
            "text/plain": "StatementMeta(SparkNB, 193, 55, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "e8b7a541-3f57-4b28-8e85-8236244431b5",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, e8b7a541-3f57-4b28-8e85-8236244431b5)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 54,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.sql(f\"SELECT count(*) FROM {lakehouse_temptable}\")\r\n",
        "display(df)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkNB",
              "session_id": "193",
              "statement_id": 56,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-01-02T04:02:33.1124462Z",
              "session_start_time": null,
              "execution_start_time": "2024-01-02T04:02:44.5530642Z",
              "execution_finish_time": "2024-01-02T04:02:46.3418221Z",
              "spark_jobs": null,
              "parent_msg_id": "d89007af-a8d9-458b-8f6c-008375697a8d"
            },
            "text/plain": "StatementMeta(SparkNB, 193, 56, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "23a56476-3f61-4407-b8f4-d35c2b6d5031",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, 23a56476-3f61-4407-b8f4-d35c2b6d5031)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 55,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## De-duplication\r\n",
        "##### Lets remove any duplicates and insert in a new view. We use the Row_number function of SQL to sort duplicates and pick up only the last updated."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(f\"\"\" CREATE OR REPLACE TEMP VIEW temptable_remduplicates \r\n",
        "AS \r\n",
        "SELECT * FROM \r\n",
        "\r\n",
        "( \r\n",
        "    SELECT ROW_NUMBER() OVER(PARTITION BY Id ORDER BY SinkModifiedOn DESC) as row_id\r\n",
        "    , * \r\n",
        "    FROM {lakehouse_temptable} \r\n",
        "\r\n",
        ") as table \r\n",
        "\r\n",
        "WHERE table.row_id = 1\"\"\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkNB",
              "session_id": "193",
              "statement_id": 57,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-01-02T04:02:33.3369115Z",
              "session_start_time": null,
              "execution_start_time": "2024-01-02T04:02:46.4567995Z",
              "execution_finish_time": "2024-01-02T04:02:47.5107347Z",
              "spark_jobs": null,
              "parent_msg_id": "73c475ca-5c93-4bc5-acda-9b97955fdbbc"
            },
            "text/plain": "StatementMeta(SparkNB, 193, 57, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 111,
          "data": {
            "text/plain": "DataFrame[]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 56,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "select count(*) from temptable_remduplicates "
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkNB",
              "session_id": "193",
              "statement_id": 58,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-01-02T04:02:33.464335Z",
              "session_start_time": null,
              "execution_start_time": "2024-01-02T04:02:47.6210747Z",
              "execution_finish_time": "2024-01-02T04:02:50.340194Z",
              "spark_jobs": null,
              "parent_msg_id": "f66d4070-3de7-44a1-88bb-70ea2a9eea4b"
            },
            "text/plain": "StatementMeta(SparkNB, 193, 58, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 57,
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "schema": {
                "type": "struct",
                "fields": [
                  {
                    "name": "count(1)",
                    "type": "long",
                    "nullable": false,
                    "metadata": {}
                  }
                ]
              },
              "data": [
                [
                  "13"
                ]
              ]
            },
            "text/plain": "<Spark SQL result set with 1 rows and 1 fields>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 57,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quality Checks\r\n",
        "##### Here we are validating that certain columns in the dataframe are unique and not null. The validate function will check all the expectations we've set up and return the results."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import great_expectations as ge\n",
        "import great_expectations.dataset.sparkdf_dataset as sd\n",
        "\n",
        "# Convert Spark DataFrame to Great Expectations Dataset\n",
        "ge_df = ge.dataset.SparkDFDataset(table_df)\n",
        "\n",
        "# Define Expectations\n",
        "\n",
        "# Not null \n",
        "ge_df.expect_column_values_to_not_be_null('createdOn')\n",
        "#ge_df.expect_column_values_to_not_be_null('creditMax')\n",
        "#ge_df.expect_column_values_to_not_be_null('contactperson')\n",
        "\n",
        "\n",
        "# Unique\n",
        "ge_df.expect_column_values_to_not_be_null('Id')\n",
        "ge_df.expect_column_values_to_be_unique('accountNum')\n",
        "\n",
        "\n",
        "# Validate Expectations\n",
        "results = ge_df.validate()\n",
        "\n",
        "print(results)\n",
        "'''"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkNB",
              "session_id": "193",
              "statement_id": 59,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-01-02T04:02:33.608369Z",
              "session_start_time": null,
              "execution_start_time": "2024-01-02T04:02:50.4599547Z",
              "execution_finish_time": "2024-01-02T04:02:50.6061666Z",
              "spark_jobs": null,
              "parent_msg_id": "bee15922-0cc0-4ebb-b9e6-0e6f1cee9391"
            },
            "text/plain": "StatementMeta(SparkNB, 193, 59, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 113,
          "data": {
            "text/plain": "\"\\nimport great_expectations as ge\\nimport great_expectations.dataset.sparkdf_dataset as sd\\n\\n# Convert Spark DataFrame to Great Expectations Dataset\\nge_df = ge.dataset.SparkDFDataset(table_df)\\n\\n# Define Expectations\\n\\n# Not null \\nge_df.expect_column_values_to_not_be_null('createdOn')\\n#ge_df.expect_column_values_to_not_be_null('creditMax')\\n#ge_df.expect_column_values_to_not_be_null('contactperson')\\n\\n\\n# Unique\\nge_df.expect_column_values_to_not_be_null('Id')\\nge_df.expect_column_values_to_be_unique('accountNum')\\n\\n\\n# Validate Expectations\\nresults = ge_df.validate()\\n\\nprint(results)\\n\""
          },
          "metadata": {}
        }
      ],
      "execution_count": 58,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enrich your silver\r\n",
        "##### At this stage, you have a deduped incremental data. You can enrich it further by making joins with other tables, or picking out columns you are interested in and discarding rest. Next, we do the merge with the target silver table. This same process of picking out incremental changes using checkpoint folders can happen for your gold tables."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge\r\n",
        "##### Make a new dataframe based on the view that holds deduped table records. This dataframe will be used in the merge process."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can remove row_id column before merging\r\n",
        "removedDupsTable = \"temptable_remduplicates\" \r\n",
        "table_temp_df = spark.sql(f\"SELECT * FROM {removedDupsTable}\")\r\n",
        "table_temp_df = table_temp_df.drop(\"row_id\")\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkNB",
              "session_id": "193",
              "statement_id": 60,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-01-02T04:02:33.7359053Z",
              "session_start_time": null,
              "execution_start_time": "2024-01-02T04:02:50.7308303Z",
              "execution_finish_time": "2024-01-02T04:02:51.2299429Z",
              "spark_jobs": null,
              "parent_msg_id": "8babe34b-e3af-4b5d-8f79-6d838f62f6ff"
            },
            "text/plain": "StatementMeta(SparkNB, 193, 60, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 59,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### We use Delta lake merge function, to either update or insert. Note the condition uses Id and IsDelete. The deleted records will come with IsDelete as true and hence will be treated as new records. You need to handle the deletes later. More info: https://docs.delta.io/latest/delta-update.html"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if Delta table exists\r\n",
        "if mssparkutils.fs.exists(path_to_target_table):\r\n",
        "    targetTable = DeltaTable.forPath(spark, path_to_target_table)\r\n",
        "else:\r\n",
        "    # Create Delta table\r\n",
        "    table_temp_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(lakehouse + \".\" + target_table)\r\n",
        "    targetTable = DeltaTable.forPath(spark, path_to_target_table)\r\n",
        "\r\n",
        "\r\n",
        "# Define merge condition\r\n",
        "merge_condition = (\r\n",
        "    \"source.Id = target.Id\"\r\n",
        ")\r\n",
        "\r\n",
        "# Execute merge\r\n",
        "(\r\n",
        " targetTable.alias(\"target\")\r\n",
        " .merge(\r\n",
        "    table_temp_df.alias(\"source\"),\r\n",
        "    merge_condition\r\n",
        "\r\n",
        " )\r\n",
        " .whenMatchedUpdateAll()\r\n",
        " .whenNotMatchedInsertAll()\r\n",
        " .execute()\r\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkNB",
              "session_id": "193",
              "statement_id": 61,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-01-02T04:02:33.8516569Z",
              "session_start_time": null,
              "execution_start_time": "2024-01-02T04:02:51.3597865Z",
              "execution_finish_time": "2024-01-02T04:03:20.1862722Z",
              "spark_jobs": null,
              "parent_msg_id": "56933910-f740-4058-bed0-c9cbab944dbc"
            },
            "text/plain": "StatementMeta(SparkNB, 193, 61, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 60,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleanup with Vacuum command\r\n",
        "##### Remove old data in the temp table by using the Vacuum command, which physically removes files from storage that are older than the retention period. This saves storage costs. You can consider using vaccum for source delta lake folders (Synapse link folders) as well once data is read. More info: https://delta.io/blog/remove-files-delta-lake-vacuum-command/"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
        "spark.sql(f\"VACUUM {lakehouse_temptable} RETAIN 0 HOURS\").show(truncate=False)\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "SparkNB",
              "session_id": "193",
              "statement_id": 62,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-01-02T04:02:33.946915Z",
              "session_start_time": null,
              "execution_start_time": "2024-01-02T04:03:20.3062882Z",
              "execution_finish_time": "2024-01-02T04:03:25.4974029Z",
              "spark_jobs": null,
              "parent_msg_id": "66e845f0-0c76-4e8f-b8f7-6eae8f021ddb"
            },
            "text/plain": "StatementMeta(SparkNB, 193, 62, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------------------------------------------------------------------+\n|path                                                                                                |\n+----------------------------------------------------------------------------------------------------+\n|abfss://synapselink-lakehouses@salabcommercedatalake.dfs.core.windows.net/lakehouse_1/vendtable_temp|\n+----------------------------------------------------------------------------------------------------+\n\n"
          ]
        }
      ],
      "execution_count": 61,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "##### The notebook demonstrates a POC of achieving incremental pipeline for data loading at scale using Checkpoints in Spark Structured Streaming. Checkpoints offer an easy to manage and efficient mechansim of loading data."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {
        "e8b7a541-3f57-4b28-8e85-8236244431b5": {
          "type": "Synapse.DataFrame",
          "sync_state": {
            "table": {
              "rows": [
                {
                  "0": "2024-01-02 03:40:38.784175",
                  "1": "2023-12-31 03:08:48.453054"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "max(SinkModifiedOn)",
                  "type": "timestamp"
                },
                {
                  "key": "1",
                  "name": "min(SinkModifiedOn)",
                  "type": "timestamp"
                }
              ],
              "truncated": false
            },
            "isSummary": false,
            "language": "scala"
          },
          "persist_state": {
            "view": {
              "type": "details",
              "tableOptions": {},
              "chartOptions": {
                "chartType": "bar",
                "aggregationType": "count",
                "categoryFieldKeys": [
                  "0"
                ],
                "seriesFieldKeys": [
                  "0"
                ],
                "isStacked": false
              }
            }
          }
        },
        "23a56476-3f61-4407-b8f4-d35c2b6d5031": {
          "type": "Synapse.DataFrame",
          "sync_state": {
            "table": {
              "rows": [
                {
                  "0": "13"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "count(1)",
                  "type": "bigint"
                }
              ],
              "truncated": false
            },
            "isSummary": false,
            "language": "scala"
          },
          "persist_state": {
            "view": {
              "type": "details",
              "tableOptions": {},
              "chartOptions": {
                "chartType": "bar",
                "aggregationType": "sum",
                "categoryFieldKeys": [],
                "seriesFieldKeys": [
                  "0"
                ],
                "isStacked": false
              }
            }
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}