{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#r \"nuget:Azure.Identity,1.1.1\"\r\n",
        "#r \"nuget:Azure.Storage.Blobs,12.6.0\""
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "patc6sx8767zd5k",
              "session_id": 13,
              "statement_id": 1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-07-01T22:00:26.0401048Z",
              "session_start_time": "2021-07-01T22:00:26.0792194Z",
              "execution_start_time": "2021-07-01T22:02:50.5682671Z",
              "execution_finish_time": "2021-07-01T22:02:56.8114375Z"
            },
            "text/plain": "StatementMeta(patc6sx8767zd5k, 13, 1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing package Azure.Identity, version 1.1.1...Installing package Azure.Storage.Blobs, version 12.6.0...Installed package Azure.Identity version 1.1.1Installed package Azure.Storage.Blobs version 12.6.0"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dynamic dataLakeConnectionInfo = new {\r\n",
        "    StorageAccountName = \"\",\r\n",
        "    Container = \"\",\r\n",
        "    EntityStoreMetadataPath = \"\",\r\n",
        "    TenantId = \"\",\r\n",
        "    AppId = \"\",\r\n",
        "    AppKey = \"\"\r\n",
        "};\r\n",
        "\r\n",
        "dynamic sqlPoolConnectionInfo = new {\r\n",
        "    PoolName = \"\",\r\n",
        "    Schema = \"\",\r\n",
        "    Database = \"\",\r\n",
        "    Username = \"\",\r\n",
        "    Password = \"\",\r\n",
        "};\r\n",
        "\r\n",
        "\r\n",
        "var sqlPoolConnectionString = $\"jdbc:sqlserver://{sqlPoolConnectionInfo.PoolName}.sql.azuresynapse.net:1433;database={sqlPoolConnectionInfo.Database};user={sqlPoolConnectionInfo.Username};password={sqlPoolConnectionInfo.Password};encrypt=true;trustServerCertificate=true;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\";\r\n",
        "\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "patc6sx8767zd5k",
              "session_id": 13,
              "statement_id": 2,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-07-01T22:05:56.7406728Z",
              "session_start_time": null,
              "execution_start_time": "2021-07-01T22:05:56.958378Z",
              "execution_finish_time": "2021-07-01T22:05:59.0180504Z"
            },
            "text/plain": ""
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dynamics Entity Store SDK"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "using System.IO;\r\n",
        "using System.IO.Compression;\r\n",
        "using Newtonsoft.Json;\r\n",
        "using Newtonsoft.Json.Linq;\r\n",
        "using Microsoft.Spark;\r\n",
        "\r\n",
        "class EntityStoreSdk {\r\n",
        "\r\n",
        "    private string SqlConnectionString {get; set;}\r\n",
        "    private Microsoft.Spark.Sql.SparkSession SparkSession {get; set;}\r\n",
        "\r\n",
        "    public EntityStoreSdk(string sqlConnectionString, Microsoft.Spark.Sql.SparkSession sparkSession)\r\n",
        "    {\r\n",
        "        this.SqlConnectionString = sqlConnectionString;\r\n",
        "        this.SparkSession = sparkSession;\r\n",
        "    }\r\n",
        "\r\n",
        "    public JObject ReadMetadata(MemoryStream memoryStream)\r\n",
        "    {\r\n",
        "        using (var zip = new ZipArchive(memoryStream, ZipArchiveMode.Read))\r\n",
        "        {\r\n",
        "            // Finds the root measurement metadata\r\n",
        "            var entryList = zip.Entries.ToList();\r\n",
        "            var measurementEntry = entryList.FirstOrDefault(e => e.FullName == \"measurement.json\");\r\n",
        "\r\n",
        "            if (measurementEntry == null)\r\n",
        "            {\r\n",
        "                throw new Exception($\"Cannot find measurement metadata file 'measurement.json' in the root folder of the file \");\r\n",
        "            }\r\n",
        "\r\n",
        "            using (var stream = measurementEntry.Open())\r\n",
        "            {\r\n",
        "                var serializer = new JsonSerializer();\r\n",
        "\r\n",
        "                using (var sr = new StreamReader(stream))\r\n",
        "                using (var jsonTextReader = new JsonTextReader(sr))\r\n",
        "                {\r\n",
        "                    dynamic measurementMetadata = JObject.Load(jsonTextReader);\r\n",
        "\r\n",
        "                    Console.WriteLine($\"Processing measurement '{measurementMetadata.Label}' ({measurementMetadata.Name})\");\r\n",
        "\r\n",
        "                    return measurementMetadata;\r\n",
        "                }\r\n",
        "            }\r\n",
        "        }\r\n",
        "\r\n",
        "        throw new Exception(\"Could not process the aggregate measurement.\");\r\n",
        "    }\r\n",
        "\r\n",
        "    public DataFrame QueryTable(string query)\r\n",
        "    {\r\n",
        "        var df = this.SparkSession.Read()\r\n",
        "              .Format(\"com.microsoft.sqlserver.jdbc.spark\")\r\n",
        "              .Option(\"url\", this.SqlConnectionString)\r\n",
        "              .Option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\r\n",
        "              .Option(\"query\", query)   // use dbtable for the entire table\r\n",
        "              .Option(\"isolationLevel\", \"READ_UNCOMMITTED\")\r\n",
        "              .Load();\r\n",
        "\r\n",
        "        return df;\r\n",
        "    }\r\n",
        "}"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "patc6sx8767zd5k",
              "session_id": 13,
              "statement_id": 15,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-07-01T22:10:09.8568477Z",
              "session_start_time": null,
              "execution_start_time": "2021-07-01T22:10:09.9776829Z",
              "execution_finish_time": "2021-07-01T22:10:12.0311317Z"
            },
            "text/plain": ""
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Aggregate Measurement Metadata"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "using global::Azure.Identity;\r\n",
        "using global::Azure.Storage.Blobs;\r\n",
        "\r\n",
        "var entityStoreSdk = new EntityStoreSdk(sqlPoolConnectionString, spark);\r\n",
        "JObject measureMetadata = null;\r\n",
        "\r\n",
        "var tokenCredential = new ClientSecretCredential(\r\n",
        "                dataLakeConnectionInfo.TenantId,\r\n",
        "                dataLakeConnectionInfo.AppId,\r\n",
        "                dataLakeConnectionInfo.AppKey);\r\n",
        "\r\n",
        "var blobEndpoint = new Uri($\"https://{dataLakeConnectionInfo.StorageAccountName}.dfs.core.windows.net\");\r\n",
        "\r\n",
        "BlobServiceClient blobServiceClient = new BlobServiceClient(blobEndpoint, tokenCredential);\r\n",
        "BlobContainerClient containerClient = blobServiceClient.GetBlobContainerClient(dataLakeConnectionInfo.Container);\r\n",
        "\r\n",
        "BlobClient blobClient = containerClient.GetBlobClient(dataLakeConnectionInfo.EntityStoreMetadataPath);\r\n",
        "if (await blobClient.ExistsAsync())\r\n",
        "{\r\n",
        "      Console.WriteLine(\"Downloading Entity Store Metadata...\");\r\n",
        "\r\n",
        "      using (var memoryStream = new MemoryStream())\r\n",
        "      {\r\n",
        "            blobClient.DownloadTo(memoryStream);\r\n",
        "            measureMetadata = entityStoreSdk.ReadMetadata(memoryStream);\r\n",
        "      }\r\n",
        "}"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "patc6sx8767zd5k",
              "session_id": 16,
              "statement_id": 1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-07-23T18:58:25.9358363Z",
              "session_start_time": "2021-07-23T18:58:25.9962502Z",
              "execution_start_time": "2021-07-23T19:00:26.7980454Z",
              "execution_finish_time": "2021-07-23T19:00:28.8531482Z"
            },
            "text/plain": ""
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query Azure Synapse Table\r\n",
        "\r\n",
        "The example below illustrates querying an Azure Synapse Table from Spark using the [SQL Server connector](https://docs.microsoft.com/en-us/sql/connect/spark/connector?view=sql-server-ver15) in the Entity Store SDK."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "var query = \"select top(100) * from Weather\";\r\n",
        "\r\n",
        "var df = spark.Read()\r\n",
        "              .Format(\"com.microsoft.sqlserver.jdbc.spark\")\r\n",
        "              .Option(\"url\", sqlPoolConnectionString)\r\n",
        "              .Option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\r\n",
        "              .Option(\"query\", query) //use dbtable for the entire table\r\n",
        "              .Load();\r\n",
        "\r\n",
        "display(df.Head(15));\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "patc6sx8767zd5k",
              "session_id": 12,
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-07-01T15:51:11.6533693Z",
              "session_start_time": null,
              "execution_start_time": "2021-07-01T15:51:11.7656437Z",
              "execution_finish_time": "2021-07-01T15:51:13.8256164Z"
            },
            "text/plain": "StatementMeta(patc6sx8767zd5k, 12, 9, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Join Fact and Dimension Tables in Azure Synapse\r\n",
        "\r\n",
        "In the example below, we'll traverse the Measurement Metadata to join fact and dimensions tables using the [SQL Server connector](https://docs.microsoft.com/en-us/sql/connect/spark/connector?view=sql-server-ver15) in the Entity Store SDK."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "var measureGroup = measureMetadata[\"MeasureGroups\"].First();\r\n",
        "\r\n",
        "var factTableDf = entityStoreSdk.QueryTable(measureGroup[\"Table\"].ToString());\r\n",
        "\r\n",
        "var selectMeasureGroupDimensionQuery = new StringBuilder($\"SELECT * FROM {measureGroup[\"Table\"].ToString()}\");\r\n",
        "foreach (var dimension in measureGroup[\"Dimensions\"])\r\n",
        "{\r\n",
        "    var dimensionRelation = dimension[\"DimensionRelations\"]?.FirstOrDefault(); // gets the first relation\r\n",
        "    selectMeasureGroupDimensionQuery.Append($\" INNER JOIN {dimensionRelation[\"Name\"].ToString()} ON {dimensionRelation[\"DimensionAttribute\"].ToString()}\");\r\n",
        "}\r\n",
        "\r\n",
        "Console.WriteLine($\"Executing query: {selectMeasureGroupDimensionQuery.ToString()}\");\r\n",
        "\r\n",
        "var starSchemaDf = entityStoreSdk.QueryTable(selectMeasureGroupDimensionQuery.ToString());\r\n",
        "\r\n",
        "display(starSchemaDf.Head(15));\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "patc6sx8767zd5k",
              "session_id": 13,
              "statement_id": 30,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-07-01T22:17:18.3413335Z",
              "session_start_time": null,
              "execution_start_time": "2021-07-01T22:17:18.5025789Z",
              "execution_finish_time": "2021-07-01T22:17:20.5688202Z"
            },
            "text/plain": "StatementMeta(patc6sx8767zd5k, 13, 30, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Store Results in Azure Synapse SQL (Dedicated Server)\r\n",
        "\r\n",
        "Azure Synapse team recommends using the [\"synapsesql\" connector](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/synapse-spark-sql-pool-import-export#use-pyspark-with-the-connector) to read/write data from Azure Synapse SQL.\r\n",
        "This connector is currently only available in Scala, so in order to use it we'll have to first store the Dataframe as a temp view (Hive Table),\r\n",
        "then use the connector in Scala to write the temp table into Azure Synapse like the cells below.\r\n",
        "\r\n",
        "Please notice that you must grant \"Storage Blob Data Contributor\" right in the storage account used by the Azure Synapse workspace.\r\n",
        "See the [documentation](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/synapse-spark-sql-pool-import-export#use-pyspark-with-the-connector) for more details.\r\n",
        "\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.CreateOrReplaceTempView(\"DestinationWeatherTable\");"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "patc6sx8767zd5k",
              "session_id": 12,
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-07-01T15:51:33.5665041Z",
              "session_start_time": null,
              "execution_start_time": "2021-07-01T15:51:33.666333Z",
              "execution_finish_time": "2021-07-01T15:51:35.7234332Z"
            },
            "text/plain": "StatementMeta(patc6sx8767zd5k, 12, 10, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\r\n",
        "val scala_df = spark.sqlContext.sql (\"select * from DestinationWeatherTable\")\r\n",
        "\r\n",
        "scala_df.write.\r\n",
        "option(Constants.SERVER, \"synapsews-dev-westus2-c30d4.database.windows.net\").\r\n",
        "synapsesql(\"SampleSQL.dbo.DestinationWeatherTable\", Constants.INTERNAL)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "patc6sx8767zd5k",
              "session_id": 12,
              "statement_id": 11,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-07-01T15:51:51.1641197Z",
              "session_start_time": null,
              "execution_start_time": "2021-07-01T15:51:51.2673847Z",
              "execution_finish_time": "2021-07-01T15:52:05.6865074Z"
            },
            "text/plain": "StatementMeta(patc6sx8767zd5k, 12, 11, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Store Results in the Data Lake in CDM Format\r\n",
        "\r\n",
        "You can use CDM Spark Connector to write the contents of the DataFrame using [CDM Spark](https://github.com/Azure/spark-cdm-connector) connector:\r\n",
        "\r\n",
        "1. Download the latest CDM Spark connector jar e.g. spark-cdm-connector-assembly-0.19.1.jar\r\n",
        "2. Upload the package in the workspace following these [instructions](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-manage-scala-packages).\r\n",
        "3. Install the package in the Spark Pool in the notebook (or in all Spark Pools by default)\r\n",
        "\r\n",
        "\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "// See examples in https://github.com/Azure/spark-cdm-connector/blob/master/samples/SparkCDMsample.scala\r\n",
        "\r\n",
        "// Implicit write\r\n",
        "df.write.format(\"com.microsoft.cdm\")\r\n",
        "  .option(\"storage\", dataLakeConnectionInfo.StorageAccountName)\r\n",
        "  .option(\"manifestPath\", dataLakeConnectionInfo.Container + \"/nestedImplicit/default.manifest.cdm.json\")\r\n",
        "  .option(\"entity\", \"NestedExampleImplicit\")\r\n",
        "  .option(\"format\", \"parquet\")\r\n",
        "  .save()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_sparkdotnet",
      "language": "C#",
      "display_name": "Synapse SparkDotNet"
    },
    "language_info": {
      "name": "csharp"
    },
    "kernel_info": {
      "name": "synapse_sparkdotnet"
    },
    "save_output": false,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}