{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Entity Store Tools V2.7"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies\r\n",
        "Install the nuget packages required for the Entity Store SDK."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#r \"nuget:Azure.Identity,1.1.1\"\r\n",
        "#r \"nuget:Azure.Storage.Blobs,12.6.0\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration\r\n",
        "Provide connection details for the data lake and sql pool."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "using Microsoft.Spark.Extensions.Azure.Synapse.Analytics.Utils;\r\n",
        "\r\n",
        "dynamic dataLakeConnectionInfo = new {\r\n",
        "    StorageAccountName = \"\",\r\n",
        "    Container = \"\",\r\n",
        "    EntityStoreMetadataPath = \"\",\r\n",
        "};\r\n",
        "\r\n",
        "dynamic sqlPoolConnectionInfo = new {\r\n",
        "    PoolName = \"\",\r\n",
        "    Schema = \"\",\r\n",
        "    Database = \"\",\r\n",
        "};\r\n",
        "\r\n",
        "\r\n",
        "var sqlPoolConnectionString = TokenLibrary.GetSecret(\"<KEYVAULT-NAME>\", \"SECRET-NAME-SQL\");\r\n",
        "\r\n",
        "string adlsConnectionString = TokenLibrary.GetSecret(\"<KEYVAULT-NAME>\", \"SECRET-NAME-ADLS\");\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dynamics Entity Store SDK"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "using System.IO;\r\n",
        "using System.IO.Compression;\r\n",
        "using Newtonsoft.Json;\r\n",
        "using Newtonsoft.Json.Linq;\r\n",
        "using Microsoft.Spark;\r\n",
        "\r\n",
        "class EntityStoreSdk {\r\n",
        "\r\n",
        "    private string SqlConnectionString {get; set;}\r\n",
        "    private Microsoft.Spark.Sql.SparkSession SparkSession {get; set;}\r\n",
        "\r\n",
        "    public EntityStoreSdk(string sqlConnectionString, Microsoft.Spark.Sql.SparkSession sparkSession)\r\n",
        "    {\r\n",
        "        this.SqlConnectionString = sqlConnectionString;\r\n",
        "        this.SparkSession = sparkSession;\r\n",
        "    }\r\n",
        "\r\n",
        "    public JObject ReadMetadata(MemoryStream memoryStream)\r\n",
        "    {\r\n",
        "        using (var zip = new ZipArchive(memoryStream, ZipArchiveMode.Read))\r\n",
        "        {\r\n",
        "            // Finds the root measurement metadata\r\n",
        "            var entryList = zip.Entries.ToList();\r\n",
        "            var measurementEntry = entryList.FirstOrDefault(e => e.FullName == \"measurement.json\");\r\n",
        "\r\n",
        "            if (measurementEntry == null)\r\n",
        "            {\r\n",
        "                throw new Exception($\"Cannot find measurement metadata file 'measurement.json' in the root folder of the file \");\r\n",
        "            }\r\n",
        "\r\n",
        "            using (var stream = measurementEntry.Open())\r\n",
        "            {\r\n",
        "                var serializer = new JsonSerializer();\r\n",
        "\r\n",
        "                using (var sr = new StreamReader(stream))\r\n",
        "                using (var jsonTextReader = new JsonTextReader(sr))\r\n",
        "                {\r\n",
        "                    dynamic measurementMetadata = JObject.Load(jsonTextReader);\r\n",
        "\r\n",
        "                    Console.WriteLine($\"Processing measurement '{measurementMetadata.Label}' ({measurementMetadata.Name})\");\r\n",
        "\r\n",
        "                    return measurementMetadata;\r\n",
        "                }\r\n",
        "            }\r\n",
        "        }\r\n",
        "\r\n",
        "        throw new Exception(\"Could not process the aggregate measurement.\");\r\n",
        "    }\r\n",
        "\r\n",
        "    public string ConvertReservedWords(dynamic dimensionAttribute)\r\n",
        "    {\r\n",
        "        HashSet<string> reservedWords = new HashSet<string>()\r\n",
        "        {\r\n",
        "            \"KEY\",\r\n",
        "            \"COMMENT\",\r\n",
        "        };\r\n",
        "\r\n",
        "        if (reservedWords.Contains(dimensionAttribute.ToString().ToUpper()))\r\n",
        "        {\r\n",
        "            return $\"{dimensionAttribute}_\";\r\n",
        "        }\r\n",
        "\r\n",
        "        return dimensionAttribute.ToString();\r\n",
        "    }\r\n",
        "\r\n",
        "    public DataFrame QueryTable(string query)\r\n",
        "    {\r\n",
        "        var df = this.SparkSession.Read()\r\n",
        "              .Format(\"com.microsoft.sqlserver.jdbc.spark\")\r\n",
        "              .Option(\"url\", this.SqlConnectionString)\r\n",
        "              .Option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\r\n",
        "              .Option(\"query\", query)   // use dbtable for the entire table\r\n",
        "              .Option(\"isolationLevel\", \"READ_UNCOMMITTED\")\r\n",
        "              .Load();\r\n",
        "\r\n",
        "        return df;\r\n",
        "    }\r\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Aggregate Measurement Metadata"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "using global::Azure.Identity;\r\n",
        "using global::Azure.Storage.Blobs;\r\n",
        "\r\n",
        "var entityStoreSdk = new EntityStoreSdk(sqlPoolConnectionString, spark);\r\n",
        "JObject measureMetadata = null;\r\n",
        "\r\n",
        "BlobServiceClient blobServiceClient = new BlobServiceClient(adlsConnectionString);\r\n",
        "BlobContainerClient containerClient = blobServiceClient.GetBlobContainerClient(dataLakeConnectionInfo.Container);\r\n",
        "\r\n",
        "BlobClient blobClient = containerClient.GetBlobClient(dataLakeConnectionInfo.EntityStoreMetadataPath);\r\n",
        "if (await blobClient.ExistsAsync())\r\n",
        "{\r\n",
        "      Console.WriteLine(\"Downloading Entity Store Metadata...\");\r\n",
        "\r\n",
        "      using (var memoryStream = new MemoryStream())\r\n",
        "      {\r\n",
        "            blobClient.DownloadTo(memoryStream);\r\n",
        "            measureMetadata = entityStoreSdk.ReadMetadata(memoryStream);\r\n",
        "      }\r\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display Temp Variable"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(measureMetadata[\"MeasureGroups\"]);"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query Azure Synapse Table\r\n",
        "\r\n",
        "The example below illustrates querying an Azure Synapse Table from Spark using the [SQL Server connector](https://docs.microsoft.com/en-us/sql/connect/spark/connector?view=sql-server-ver15) in the Entity Store SDK."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "/*var query = \"select top(100) * from dbo.Weather\";\r\n",
        "\r\n",
        "var df = spark.Read()\r\n",
        "              .Format(\"com.microsoft.sqlserver.jdbc.spark\")\r\n",
        "              .Option(\"url\", sqlPoolConnectionString)\r\n",
        "              .Option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\r\n",
        "              .Option(\"query\", query) //use dbtable for the entire table\r\n",
        "              .Load();\r\n",
        "\r\n",
        "display(df.Head(15));\r\n",
        "\r\n",
        "df.Coalesce(1).Write()\r\n",
        "        .Option(\"spark.synapse.linkedService\", \"<ADLS-LinkedService-Name>\")\r\n",
        "        .Option(\"header\", \"true\")\r\n",
        "        .Mode(SaveMode.Overwrite)\r\n",
        "        .Csv($\"abfss://{dataLakeConnectionInfo.Container}@{dataLakeConnectionInfo.StorageAccountName}.dfs.core.windows.net/Test/DestinationWeatherTable\");*/"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Join Fact and Dimension Tables in Azure Synapse (Data Cooking)\r\n",
        "\r\n",
        "In the example below, we'll traverse the Measurement Metadata to join fact and dimensions tables using the [SQL Server connector](https://docs.microsoft.com/en-us/sql/connect/spark/connector?view=sql-server-ver15) in the Entity Store SDK."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "var entityStoreSdk = new EntityStoreSdk(sqlPoolConnectionString, spark);\r\n",
        "\r\n",
        "try {\r\n",
        "    // Iterate over each MeasureGroup/FactTable.\r\n",
        "    foreach (var measureGroup in measureMetadata[\"MeasureGroups\"])\r\n",
        "    {\r\n",
        "        HashSet<string> columnsList = new HashSet<string>();\r\n",
        "\r\n",
        "        // Generate SELECT statement\r\n",
        "        var selectMeasureGroupnQuery = $\"SELECT \";\r\n",
        "        \r\n",
        "        // Add Attributes\r\n",
        "        foreach (var attribute in measureGroup[\"Attributes\"])\r\n",
        "        {\r\n",
        "            if(columnsList.Contains(attribute[\"Name\"].ToString()))               \r\n",
        "            {\r\n",
        "                continue;\r\n",
        "\r\n",
        "            }\r\n",
        "            columnsList.Add(attribute[\"Name\"].ToString());\r\n",
        "            var modifiedAttribute = entityStoreSdk.ConvertReservedWords(attribute[\"Name\"].ToString());\r\n",
        "            selectMeasureGroupnQuery += $\"{modifiedAttribute}, \";\r\n",
        "        }\r\n",
        "\r\n",
        "        // Add Measures\r\n",
        "        foreach (var measure in measureGroup[\"Measures\"])\r\n",
        "        {\r\n",
        "            if(columnsList.Contains(measure[\"Name\"].ToString()))               \r\n",
        "            {\r\n",
        "                continue;\r\n",
        "\r\n",
        "            }\r\n",
        "            columnsList.Add(measure[\"Name\"].ToString());\r\n",
        "            selectMeasureGroupnQuery += $\"{measure[\"Name\"]}, \";\r\n",
        "        }\r\n",
        "\r\n",
        "        // Add RelatedField from Dimensions\r\n",
        "        foreach (var dimension in measureGroup[\"Dimensions\"])\r\n",
        "        {\r\n",
        "            foreach (var dimensionRelation in dimension[\"DimensionRelations\"])\r\n",
        "            {\r\n",
        "                if(columnsList.Contains(dimensionRelation[\"Constraints\"][0][\"RelatedField\"].ToString()))               \r\n",
        "                {\r\n",
        "                    continue;\r\n",
        "\r\n",
        "                }\r\n",
        "                columnsList.Add(dimensionRelation[\"Constraints\"][0][\"RelatedField\"].ToString());\r\n",
        "                selectMeasureGroupnQuery += $\"{dimensionRelation[\"Constraints\"][0][\"RelatedField\"]}, \";\r\n",
        "            }\r\n",
        "        }\r\n",
        "\r\n",
        "        selectMeasureGroupnQuery = selectMeasureGroupnQuery.Remove(selectMeasureGroupnQuery.Length - 2);\r\n",
        "        selectMeasureGroupnQuery += $\" FROM [DBO].[{measureMetadata[\"Name\"]}_{measureGroup[\"Name\"]}]\";\r\n",
        "\r\n",
        "        Console.WriteLine($\"Executing query: {selectMeasureGroupnQuery.ToString()}\\n\");\r\n",
        "\r\n",
        "        // Cook data\r\n",
        "        var starSchemaDf = entityStoreSdk.QueryTable(selectMeasureGroupnQuery.ToString());\r\n",
        "\r\n",
        "        display(starSchemaDf);\r\n",
        "\r\n",
        "        // Write CDM files in ADLS\r\n",
        "        starSchemaDf.Write().Format(\"com.microsoft.cdm\")\r\n",
        "            .Option(\"storage\", dataLakeConnectionInfo.StorageAccountName + \".dfs.core.windows.net\")\r\n",
        "            .Option(\"manifestPath\", dataLakeConnectionInfo.Container + \"/AggregateMeasurements/default.manifest.cdm.json\")\r\n",
        "            .Option(\"entity\", $\"{measureMetadata[\"Name\"]}_{measureGroup[\"Name\"]}\")\r\n",
        "            .Option(\"format\", \"csv\")\r\n",
        "            .Mode(SaveMode.Overwrite)\r\n",
        "            .Save();\r\n",
        "\r\n",
        "        // Write data CSV files in ADLS\r\n",
        "        /*starSchemaDf.Coalesce(1).Write()\r\n",
        "                .Option(\"spark.synapse.linkedService\", \"<ADLS_LinkedServiceName>\")\r\n",
        "                .Option(\"header\", \"true\")\r\n",
        "                .Mode(SaveMode.Overwrite)\r\n",
        "                .Csv($\"abfss://{dataLakeConnectionInfo.Container}@{dataLakeConnectionInfo.StorageAccountName}.dfs.core.windows.net/AggregateMeasurements/{measureMetadata[\"Name\"]}_{measureGroup[\"Name\"]}\");*/\r\n",
        "    }\r\n",
        "}\r\n",
        "catch ( Exception ex ) {\r\n",
        "    Console.WriteLine($\"Error processing aggregate measurement: {measureMetadata[\"Name\"].ToString()} \\n {ex.Message}\");\r\n",
        "}\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Store Results in Azure Synapse SQL (Dedicated Server)\r\n",
        "\r\n",
        "Azure Synapse team recommends using the [\"synapsesql\" connector](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/synapse-spark-sql-pool-import-export#use-pyspark-with-the-connector) to read/write data from Azure Synapse SQL.\r\n",
        "This connector is currently only available in Scala, so in order to use it we'll have to first store the Dataframe as a temp view (Hive Table),\r\n",
        "then use the connector in Scala to write the temp table into Azure Synapse like the cells below.\r\n",
        "\r\n",
        "Please notice that you must grant \"Storage Blob Data Contributor\" right in the storage account used by the Azure Synapse workspace.\r\n",
        "See the [documentation](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/synapse-spark-sql-pool-import-export#use-pyspark-with-the-connector) for more details.\r\n",
        "\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\r\n",
        "/*\r\n",
        "val scala_df = spark.sqlContext.sql (\"select * from tempBudgetRegisterEntryEntity\")\r\n",
        "\r\n",
        "scala_df.write.\r\n",
        "option(Constants.SERVER, \"synapsews-dev-westus2-c30d4.database.windows.net\").\r\n",
        "synapsesql(\"SampleSQL.dbo.BudgetRegisterEntryEntity_MATERIALIZED\", Constants.INTERNAL)\r\n",
        "*/"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Store Results in the Data Lake in CDM Format\r\n",
        "\r\n",
        "You can use CDM Spark Connector to write the contents of the DataFrame using [CDM Spark](https://github.com/Azure/spark-cdm-connector) connector:\r\n",
        "\r\n",
        "1. Download the latest CDM Spark connector jar e.g. spark-cdm-connector-assembly-0.19.1.jar\r\n",
        "2. Upload the package in the workspace following these [instructions](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-manage-scala-packages).\r\n",
        "3. Install the package in the Spark Pool in the notebook (or in all Spark Pools by default)\r\n",
        "\r\n",
        "\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "// See examples in https://github.com/Azure/spark-cdm-connector/blob/master/samples/SparkCDMsample.scala\r\n",
        "\r\n",
        "// Implicit write example\r\n",
        "/*starSchemaDf.Write().Format(\"com.microsoft.cdm\")\r\n",
        "  .Option(\"storage\", dataLakeConnectionInfo.StorageAccountName + \".dfs.core.windows.net\")\r\n",
        "  .Option(\"manifestPath\", dataLakeConnectionInfo.Container + \"/nestedImplicit/default.manifest.cdm.json\")\r\n",
        "  .Option(\"entity\", \"NestedExampleImplicit\")\r\n",
        "  .Option(\"format\", \"parquet\")\r\n",
        "  .Mode(SaveMode.Append)\r\n",
        "  .Save();*/"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_sparkdotnet",
      "display_name": "csharp"
    },
    "language_info": {
      "name": "csharp"
    },
    "description": null,
    "save_output": false,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}